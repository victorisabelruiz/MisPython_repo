{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0\n",
       "0  2\n",
       "1  3\n",
       "2  4"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "df = pd.DataFrame([2,3,4])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = pyspark.SparkContext()\n",
    "\n",
    "#con esto podemos trabajar con los RDD cutres\n",
    "sc.parallelize([1,2,3]).count()\n",
    "\n",
    "# si creamos un contexto de SQL (a partir del SparkContext)\n",
    "# entonces ya podemos usar todas las funcionalidades de los dataframe de SQL\n",
    "sql = pyspark.SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leer y escribir: parquet, csv, ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para que funcionara la escritura de parquets hubo que añadir una variable de entorno que apuntara a un .exe que se descarga aqui:\n",
    "\n",
    "https://github.com/steveloughran/winutils/blob/master/hadoop-2.7.1/bin/winutils.exe\n",
    "\n",
    "se pega en un directorio C:/hadoop/bin/wintutils.exe\n",
    "\n",
    "se añade HADOOP_HOME que apunte a: C:/hadoop/\n",
    "\n",
    "RESETEAMos el notebook y ya funciona\n",
    "\n",
    "YA FUNCIONA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------------------------+--------------------+--------------------+------------------+\n",
      "|_c0 |_c1                        |_c2                 |_c3                 |_c4               |\n",
      "+----+---------------------------+--------------------+--------------------+------------------+\n",
      "|Year|Industry_aggregation_NZSIOC|Industry_code_NZSIOC|Industry_name_NZSIOC|Units             |\n",
      "|2019|Level 1                    |99999               |All industries      |Dollars (millions)|\n",
      "|2019|Level 1                    |99999               |All industries      |Dollars (millions)|\n",
      "|2019|Level 1                    |99999               |All industries      |Dollars (millions)|\n",
      "|2019|Level 1                    |99999               |All industries      |Dollars (millions)|\n",
      "+----+---------------------------+--------------------+--------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.\\\n",
    "            csv(\"C:/Users/vr255019/Downloads/annual-enterprise-survey-2019-financial-year-provisional-csv.csv\").\\\n",
    "            select('_c0','_c1','_c2','_c3','_c4')\n",
    "df.show(5,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.parquet(\"DATOS/tabla1\") # este directorio esta en el mismo lugar que los notebooks. Datos es una subcarpeta y tabla1 el nombre del parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.saveAsTable(\"people5\") # escrito asi te lo deja en raiz_notebooks/spark-warehouse con el nombre people5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.saveAsTable(\"people6\") # escrito asi te lo deja en raiz_notebooks/spark-warehouse con el nombre people5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-----------+\n",
      "|database|tableName|isTemporary|\n",
      "+--------+---------+-----------+\n",
      "| default|  people5|      false|\n",
      "| default|  people6|      false|\n",
      "+--------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show tables\").show(4) # asi podemos ver lo que hay en raiz_notebooks/spark-warehouse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tech Ant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "from pathlib import Path\n",
    "\n",
    "DEFAULT_CONF_ENV = \"discovery\"\n",
    "\n",
    "def load_default_tables():\n",
    "\n",
    "    tables_conf = \"C:/Users/vr255019/OneDrive - Teradata/Desktop/new_training/GEAOSP_UC34_Quality_DS/tech_ant/remedies/conf/discovery/tables.json\"\n",
    "    return load_json(tables_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[key: string, value: string]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_yarn_connection(app_name):\n",
    "    conf = pyspark.SparkConf()    \n",
    "    return SparkSession.builder.config(conf=conf).enableHiveSupport().appName(app_name).getOrCreate()\n",
    "\n",
    "spark = create_yarn_connection('Bau_technical_anticipation')\n",
    "spark.sql('set spark.sql.hive.convertMetastoreParquet=false')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "def load_json(path):\n",
    "\n",
    "    with Path(path).open(\"r\") as fp:\n",
    "        result = json.load(fp)\n",
    "   \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "people5\n"
     ]
    }
   ],
   "source": [
    "class Pipeline(object):\n",
    "   \n",
    "    def __init__(self, spark_session, pipeline_conf):\n",
    "   \n",
    "        #self.logger = logging.getLogger(__name__)\n",
    "        self.spark = spark_session\n",
    "        self.conf = pipeline_conf if isinstance(pipeline_conf, dict) else 'load_json(pipeline_conf)'\n",
    "\n",
    "class ModelPipeline(Pipeline):\n",
    "\n",
    "    def __init__(self, spark_session, pipeline_conf):\n",
    "\n",
    "        super().__init__(spark_session, pipeline_conf)\n",
    "#         self.logger = logging.getLogger(__name__)\n",
    "#         self.model_transformation_handler = self.get_model_transformer()(spark_session=spark_session, pipeline_conf=self.conf[\"model_transformation\"])\n",
    "#         self.scinet = SciNet(self.conf[\"scinet\"])\n",
    "\n",
    "class PredictModel(ModelPipeline):\n",
    "  \n",
    "    def __init__(self, spark_session, pipeline_conf):\n",
    "\n",
    "        super().__init__(spark_session, pipeline_conf)\n",
    "        self.tables = self.conf[\"tables\"] if \"tables\" in self.conf else load_default_tables()\n",
    "        \n",
    "    def drop_table(self, table_name):\n",
    "        print(table_name)\n",
    "        return self.spark.sql(\"DROP TABLE IF EXISTS {table}\".format(table=table_name))\n",
    "\n",
    "    def drop_scoring_hist_table(self):\n",
    "      \n",
    "        self.drop_table(table_name=self.tables[\"scoring_hist_table\"])\n",
    "\n",
    "    def drop_predictions_fts_hist_table(self):\n",
    "        \"\"\"Helper function to drop the predictions dataset table set by conf\n",
    "        \"\"\"\n",
    "        self.drop_table(table_name=self.tables[\"predictions_fts_hist_table\"])\n",
    "\n",
    "predict_pipe = PredictModel(spark_session=spark, pipeline_conf='PREDICT_CONF')\n",
    "predict_pipe.drop_predictions_fts_hist_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "493.2px",
    "left": "56px",
    "top": "110px",
    "width": "212px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
